(pissa) [haoxiang.jiang@d6-hpc-sjtugpu-002 ~/PiSSA/scripts]$ bash run_lora.sh 
[2024-07-18 02:22:24,177] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-07-18 02:22:39,992] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-18 02:22:39,992] [INFO] [runner.py:568:main] cmd = /hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=54633 --enable_each_rank_log=None /hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py --deepspeed /hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json --model_name_or_path /hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b --use_lora True --init_lora_weights True --target_modules q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj --lora_rank 128 --lora_alpha 128 --lora_dropout 0 --data_path /hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa --dataset_field query response --dataset_split train[:100000] --output_dir /hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora --num_train_epochs 1 --model_max_length 512 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --save_strategy steps --save_steps 100 --save_total_limit 100 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --logging_steps 1 --lr_scheduler_type cosine --report_to tensorboard --merge True
[2024-07-18 02:22:42,874] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-07-18 02:22:47,323] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-18 02:22:47,323] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-18 02:22:47,323] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-18 02:22:47,323] [INFO] [launch.py:164:main] dist_world_size=8
[2024-07-18 02:22:47,323] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-18 02:22:47,325] [INFO] [launch.py:256:main] process 134658 spawned with command: ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=0', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True']
[2024-07-18 02:22:47,326] [INFO] [launch.py:256:main] process 134659 spawned with command: ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=1', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True']
[2024-07-18 02:22:47,327] [INFO] [launch.py:256:main] process 134660 spawned with command: ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=2', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True']
[2024-07-18 02:22:47,327] [INFO] [launch.py:256:main] process 134661 spawned with command: ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=3', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True']
[2024-07-18 02:22:47,328] [INFO] [launch.py:256:main] process 134662 spawned with command: ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=4', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True']
[2024-07-18 02:22:47,330] [INFO] [launch.py:256:main] process 134663 spawned with command: ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=5', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True']
[2024-07-18 02:22:47,331] [INFO] [launch.py:256:main] process 134664 spawned with command: ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=6', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True']
[2024-07-18 02:22:47,332] [INFO] [launch.py:256:main] process 134665 spawned with command: ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=7', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True']
[2024-07-18 02:22:57,626] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-18 02:22:57,631] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-18 02:22:57,631] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-18 02:22:57,631] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-18 02:22:57,633] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-18 02:22:57,634] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-18 02:22:57,635] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-18 02:22:57,640] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /hpc_stor03/sjtu_home/haoxiang.jiang/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-07-18 02:22:59,707] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-18 02:22:59,707] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-18 02:22:59,708] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-18 02:22:59,708] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-18 02:22:59,712] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-18 02:22:59,716] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-18 02:22:59,717] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-18 02:22:59,725] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-18 02:22:59,731] [INFO] [comm.py:637:init_distributed] cdb=None
[WARNING|logging.py:313] 2024-07-18 02:23:00,632 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:313] 2024-07-18 02:23:00,657 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:313] 2024-07-18 02:23:01,659 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:313] 2024-07-18 02:23:01,769 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:313] 2024-07-18 02:23:01,785 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:313] 2024-07-18 02:23:01,814 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:313] 2024-07-18 02:23:01,834 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:313] 2024-07-18 02:23:01,846 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:54<00:00, 13.59s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:54<00:00, 13.59s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:54<00:00, 13.59s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:54<00:00, 13.59s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:54<00:00, 13.59s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:54<00:00, 13.60s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:54<00:00, 13.59s/it]
[2024-07-18 02:23:57,084] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 134658
[2024-07-18 02:24:03,296] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 134659
[2024-07-18 02:24:03,297] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 134660
[2024-07-18 02:24:07,156] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 134661
Generating train split: 0 examples [00:00, ? examples/s][2024-07-18 02:24:12,019] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 134662
[2024-07-18 02:24:14,654] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 134663
[2024-07-18 02:24:17,490] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 134664
Generating train split: 0 examples [00:00, ? examples/s][2024-07-18 02:24:20,490] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 134665
[2024-07-18 02:24:23,660] [ERROR] [launch.py:325:sigkill_handler] ['/hpc_stor03/sjtu_home/haoxiang.jiang/anaconda3/envs/pissa/bin/python3.10', '-u', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/pissa.py', '--local_rank=7', '--deepspeed', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/configs/ds_config_zero2_no_offload.json', '--model_name_or_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/llama3-8b', '--use_lora', 'True', '--init_lora_weights', 'True', '--target_modules', 'q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj', '--lora_rank', '128', '--lora_alpha', '128', '--lora_dropout', '0', '--data_path', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/mathqa', '--dataset_field', 'query', 'response', '--dataset_split', 'train[:100000]', '--output_dir', '/hpc_stor03/sjtu_home/haoxiang.jiang/PiSSA/outputs/lora', '--num_train_epochs', '1', '--model_max_length', '512', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--logging_steps', '1', '--lr_scheduler_type', 'cosine', '--report_to', 'tensorboard', '--merge', 'True'] exits with return code = -9